# ğŸ§© REPORTE TÃ‰CNICO â€“ Fallo en el Scraper Fandom + Requerimientos de OptimizaciÃ³n y Blindaje

## ğŸ“Œ 1. DescripciÃ³n del problema
Durante la ejecuciÃ³n del pipeline en `orchestrator.py`, el scraper Fandom falla con un error:

```
HTTPError: 404 Client Error: Not Found for url: https://naruto.fandom.com/wiki/List_of_Story_Arcs
```

Este error provoca que el mÃ³dulo `tenacity` reintente varias veces y finalmente el programa completo termina con:

```
tenacity.RetryError
```

Deteniendo todo el pipeline.

---

## ğŸ“Œ 2. Causa raÃ­z confirmada
El scraper apunta a una URL **incorrecta / obsoleta**, la cual ya no existe en Fandom:

```
https://naruto.fandom.com/wiki/List_of_Story_Arcs
```

Fandom cambiÃ³ la estructura y esa URL ahora responde **404 Not Found**.

Las URLs vÃ¡lidas actualmente son:

```
https://naruto.fandom.com/wiki/Story_Arcs
https://naruto.fandom.com/wiki/Category:Story_Arcs
https://naruto.fandom.com/es/wiki/Arcos_Argumentales
```

---

## ğŸ“Œ 3. Impacto en el pipeline
* Bloquea todo el flujo `scraping â†’ cleaning â†’ metrics â†’ report`.
* Tenacity agota todos los reintentos.
* No se generan:
  * `fandom_arcs.tsv`
  * `naruto_analysis.db`
  * `naruto_analysis.xlsx`
  * `metrics.json`
  * `report.html`

Sistema queda inutilizable.

---

# ğŸ›¡ï¸ 4. Requerimientos de BLINDAJE del sistema (Codex debe implementarlo)

## âœ”ï¸ 4.1. Manejo inteligente de errores HTTP (soft-fail, no hard-crash)
Nueva polÃ­tica:

* Si un scraper falla (404, 500, timeout), el pipeline **NO debe detenerse**.
* El sistema debe:
  1. Registrar un warning.
  2. Crear un archivo vacÃ­o o parcial.
  3. Continuar con las demÃ¡s fuentes.

Ejemplo:

```
WARNING: FandomScraper failed (404). Continuing pipeline with partial data.
```

---

## âœ”ï¸ 4.2. URLs dinÃ¡micas + fallback automÃ¡tico
Agregar al scraper:

```python
FANDOM_URLS = [
    "https://naruto.fandom.com/wiki/Story_Arcs",
    "https://naruto.fandom.com/wiki/Category:Story_Arcs",
    "https://naruto.fandom.com/es/wiki/Arcos_Argumentales"  # fallback internacional
]
```

Implementar:

1. Intentar cada URL en orden.
2. Si todas fallan â†’ registrar warning y devolver lista vacÃ­a.

---

## âœ”ï¸ 4.3. Control de reintentos mÃ¡s inteligente
Cambiar tenacity:

* Retries: 3 â†’ 1
* Delay: 2s â†’ 0.2s
* Stop on 404 immediately (no reintento)

---

## âœ”ï¸ 4.4. Blindaje contra cambios en HTML
Fandom cambia DOM frecuentemente.

Implementar:

* Selectores mÃºltiples.
* Si falla el principal, intentar un selector de fallback.
* Si falla todo â†’ warning + continuar.

---

# âš¡ 5. Requerimientos de Velocidad y OptimizaciÃ³n

## âœ”ï¸ 5.1. Evitar parsear HTML completo
Usar:

```python
soup = BeautifulSoup(response.text, "lxml")
```

(lxml es 5â€“30x mÃ¡s rÃ¡pido que html.parser)

---

## âœ”ï¸ 5.2. SesiÃ³n persistente de requests
Reemplazar:

```python
requests.get(...)
```

Por:

```python
session = requests.Session()
session.get(...)
```

Beneficio:

* Reduce overhead de TCP/TLS hasta 40%
* MÃ¡s rÃ¡pido y estable

---

## âœ”ï¸ 5.3. Cache local
Agregar opciÃ³n:

* Guardar HTML de Fandom/MAL/Tropes
* Si existe y `--use-cache` â†’ no scrapea

Evita desgaste y acelera.

---

## âœ”ï¸ 5.4. AceleraciÃ³n del pipeline
Secuencias actuales son secuenciales.
Mejorar:

* Paralelizar scrapers con `asyncio` o `ThreadPoolExecutor`.

---

# ğŸ§¼ 6. Requerimientos de Ligereza (reducir peso)

## âœ”ï¸ Quitar dependencias innecesarias
WeasyPrint â€” opcional
Selenium â€” cargar solo si IMDb estÃ¡ activado
Matplotlib â€” cargar solo si grÃ¡fico estÃ¡ habilitado

Implementar:

```python
if args.imdb:
    import selenium
```

---

## âœ”ï¸ Archivos grandes en `data/`

* Comprimir TSV a `.gz`
* Comprimir cachÃ©s HTML

ReducciÃ³n esperada: **65%â€“90%**

---

# ğŸ§ª 7. Reproducibilidad (para Codex)

## Para reproducir error:

```powershell
git clone https://github.com/Adal612Git/TSUKI-NO-ME-v3.git
cd TSUKI-NO-ME-v3
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
python orchestrator.py
```

âŒ Explota en:

```
FandomScraper.fetch()
404 Not Found
RetryError
```

---

# ğŸ§© 8. QuÃ© debe entregar Codex como resultado

### âœ” Fix del FandomScraper (URLs nuevas + fallback)

### âœ” Blindaje completo en scrapers (manejo de errores suave)

### âœ” Pipeline que NO se detiene por fallas externas

### âœ” Velocidad mejorada (sessions, parallel scraping)

### âœ” Ligereza y carga dinÃ¡mica de mÃ³dulos

### âœ” Logs profesionales (INFO/WARN/ERROR)

### âœ” ActualizaciÃ³n del README

### âœ” Tests mÃ­nimos de scraping

### âœ” Compatibilidad con narrativas futuras (PokÃ©mon, One Piece, DBZ, Shingeki)

---

# ğŸš€ 9. Texto FINAL para pegarle a Codex

(Copia y pega esto tal cual)

---

**INSTRUCCIONES PARA CODEX:**

Corrige y mejora TSUKI-NO-ME v3 con los siguientes requerimientos:

1. **Fix crÃ­tico:**
   El scraper Fandom usa una URL obsoleta.
   Cambiar a un sistema de URLs dinÃ¡micas con fallback:

   * [https://naruto.fandom.com/wiki/Story_Arcs](https://naruto.fandom.com/wiki/Story_Arcs)
   * [https://naruto.fandom.com/wiki/Category:Story_Arcs](https://naruto.fandom.com/wiki/Category:Story_Arcs)
   * [https://naruto.fandom.com/es/wiki/Arcos_Argumentales](https://naruto.fandom.com/es/wiki/Arcos_Argumentales)

2. **Blindaje:**

   * Manejar errores HTTP sin interrumpir pipeline.
   * Si un scraper falla, registrar warning y seguir.
   * Control de reintentos inteligente: reintentar 1 vez, evitar reintentos en 404.
   * Selectores HTML con mÃºltiples fallback.

3. **OptimizaciÃ³n:**

   * Reemplazar `requests.get` por `requests.Session`.
   * Usar `lxml` para parsing HTML.
   * Agregar paralelismo en scrapers con ThreadPoolExecutor.
   * Cargar Selenium solo si IMDb es requerido.
   * Reducir dependencias obligatorias.

4. **Ligereza:**

   * Comprimir TSV y HTML cache como `.gz`.
   * Crear modo `--minimal` para correr sin PDFs ni grÃ¡ficos.

5. **Entrega:**

   * CÃ³digo actualizado.
   * README actualizado con nuevas instrucciones.
   * Pruebas mÃ­nimas de scraping.
   * Logs mejorados.

---

Ricardo, esto es EXACTAMENTE lo que Codex necesita para arreglar el sistema y dejarlo god-tier.

Si quieres, te preparo **directamente un archivo .md** para subirlo al repo como `SPEC_IMPROVEMENTS.md`.
